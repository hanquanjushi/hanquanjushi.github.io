---
---

@inproceedings{10.1145/3552326.3587446,
author = {Qiu, Ziyue and Yang, Juncheng and Zhang, Juncheng and Li, Cheng and Ma, Xiaosong and Chen, Qi and Yang, Mao and Xu, Yinlong},
title = {FrozenHot Cache: Rethinking Cache Management for Modern Hardware},
year = {2023},
isbn = {9781450394871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3552326.3587446},
doi = {10.1145/3552326.3587446},
abstract = {Caching is crucial for accelerating data access, employed as a ubiquitous design in modern systems at many parts of computer systems. With increasing core count, and shrinking latency gap between cache and modern storage devices, hit-path scalability becomes increasingly critical. However, existing production in-memory caches often use list-based management with promotion on each cache hit, which requires extensive locking and poses a significant overhead for scaling beyond a few cores. Moreover, existing techniques for improving scalability either (1) only focus on the indexing structure and do not improve cache management scalability, or (2) sacrifice efficiency or miss-path scalability.Inspired by highly skewed data popularity and short-term hotspot stability in cache workloads, we propose Frozen-Hot, a generic approach to improve the scalability of list-based caches. FrozenHot partitions the cache space into two parts: a frozen cache and a dynamic cache. The frozen cache serves requests for hot objects with minimal latency by eliminating promotion and locking, while the latter leverages the existing cache design to achieve workload adaptivity. We built FrozenHot as a library that can be easily integrated into existing systems. We demonstrate its performance by enabling FrozenHot in two production systems: HHVM and RocksDB using under 100 lines of code. Evaluated using production traces from MSR and Twitter, FrozenHot improves the throughput of three baseline cache algorithms by up to 551%. Compared to stock RocksDB, FrozenHot-enhanced RocksDB shows a higher throughput on all YCSB workloads with up to 90% increase, as well as reduced tail latency.},
booktitle = {Proceedings of the Eighteenth European Conference on Computer Systems},
pages = {557–573},
numpages = {17},
keywords = {concurrency algorithm, storage, performance scalability, caching},
location = {Rome, Italy},
series = {EuroSys '23},
pdf = {https://dl.acm.org/doi/abs/10.1145/3552326.3587446},
code = {https://github.com/ziyueqiu/FrozenHot},
html = {https://youtu.be/SGLLFqT3KsI},
slides = {FrozenHot-slides.pdf},
preview = {FrozenHot-fig.png},
}

@inproceedings{10.1145/3593856.3595887,
author = {Yang, Juncheng and Qiu, Ziyue and Zhang, Yazhuo and Yue, Yao and Rashmi, K. V.},
title = {FIFO Can Be Better than LRU: The Power of Lazy Promotion and Quick Demotion},
year = {2023},
isbn = {9798400701955},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593856.3595887},
doi = {10.1145/3593856.3595887},
abstract = {LRU has been the basis of cache eviction algorithms for decades, with a plethora of innovations on improving LRU's miss ratio and throughput. While it is well-known that FIFO-based eviction algorithms provide significantly better throughput and scalability, they lag behind LRU on miss ratio, thus, cache efficiency.We performed a large-scale simulation study using 5307 block and web cache workloads collected in the past two decades. We find that contrary to what common wisdom suggests, some FIFO-based algorithms, such as FIFO-Reinsertion (or CLOCK), are, in fact, more efficient (have a lower miss ratio) than LRU. Moreover, we find that qick demotion --- evicting most new objects very quickly --- is critical for cache efficiency. We show that when enhanced by qick demotion, not only can state-of-the-art algorithms be more efficient, a simple FIFO-based algorithm can outperform five complex state-of-the-art in terms of miss ratio.},
booktitle = {Proceedings of the 19th Workshop on Hot Topics in Operating Systems},
pages = {70–79},
numpages = {10},
location = {Providence, RI, USA},
series = {HOTOS '23},
pdf = {https://dl.acm.org/doi/abs/10.1145/3593856.3595887}
}

@inproceedings{10.1145/3600006.3613147,
author = {Yang, Juncheng and Zhang, Yazhuo and Qiu, Ziyue and Yue, Yao and Vinayak, Rashmi},
title = {FIFO Queues Are All You Need for Cache Eviction},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613147},
doi = {10.1145/3600006.3613147},
abstract = {As a cache eviction algorithm, FIFO has a lot of attractive properties, such as simplicity, speed, scalability, and flash-friendliness. The most prominent criticism of FIFO is its low efficiency (high miss ratio).In this work, we demonstrate a simple, scalable FIFO-based algorithm with three static queues (S3-FIFO). Evaluated on 6594 cache traces from 14 datasets, we show that S3-FIFO has lower miss ratios than state-of-the-art algorithms across traces. Moreover, S3-FIFO's efficiency is robust --- it has the lowest mean miss ratio on 10 of the 14 datasets. FIFO queues enable S3-FIFO to achieve good scalability with 6\texttimes{} higher throughput compared to optimized LRU at 16 threads.Our insight is that most objects in skewed workloads will only be accessed once in a short window, so it is critical to evict them early (also called quick demotion). The key of S3-FIFO is a small FIFO queue that filters out most objects from entering the main cache, which provides a guaranteed demotion speed and high demotion precision.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {130–149},
numpages = {20},
location = {Koblenz, Germany},
series = {SOSP '23},
pdf = {https://jasony.me/publication/sosp23-s3fifo.pdf}
}

@article{tang2023rethinking,
  title={Rethinking the Cloudonomics of Efficient I/O for Data-Intensive Analytics Applications},
  author={Tang, Chunxu and Wang, Yi and Fan, Bin and Wang, Beinan and Chen, Shouwei and Qiu, Ziyue and Liang, Chen and Zhao, Jing and Zhu, Yu and Chen, Mingmin and others},
  journal={arXiv preprint arXiv:2311.00156},
  year={2023}
  url = {https://arxiv.org/abs/2311.00156}
}